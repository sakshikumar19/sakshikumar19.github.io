<This is only a draft>

Key learnings:
- first step is always prompting
- choose the right model. Base models are not for chatting
- Intruct fine tune if needed
- Inference API is not good for data privacy.
- Start with RAG, but have a quality annotation process for building up your fine-tuning data set. Fine tune, then try the fine tuned model with RAG. But always keep the feedback loop for building the tuning data.
- Look up and spend time on SOTA techniques before getting started!

- Why search is better than fine-tuning
GPT can learn knowledge in two ways:
Via model weights (i.e., fine-tune the model on a training set)
Via model inputs (i.e., insert the knowledge into an input message)

Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.

As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read
